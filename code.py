# -*- coding: utf-8 -*-
"""code.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1y6kGT8dhU3rpJU5NKLKIpoVz8MJYEKj0

# Road Damage Detection: A Comparison of YOLOv8 and Faster R-CNN
"""
'''
!pip install -U 'git+https://github.com/facebookresearch/detectron2.git'
!pip install tensorflow==2.12.0 tensorflow-estimator==2.12.0 tf_slim protobuf==3.20.3 --quiet
!pip install tqdm lxml pycocotools
'''

"""## Code Significance Overview

This section performs two important setup operations for running deep learning workflows in Google Colab:

### 1. Mounting Google Drive
Mounting Google Drive allows seamless access to files and folders stored in your Drive directly from the Colab environment. The `force_remount=True` option ensures that even if the Drive was already mounted, it will be refreshed and reconnected cleanly.

### 2. Installing Core Dependencies
The script installs three core libraries:
- **Torch**: The PyTorch library used for building and training deep learning models.
- **Torchvision**: Provides utilities for image transformations and pretrained vision models.
- **Ultralytics**: A high-level library that includes the YOLOv8 framework, enabling easy training, validation, and deployment of object detection models with a few lines of code.

These installations ensure that the environment is equipped with the latest versions of tools needed for building YOLO-based object detection pipelines.
"""

from google.colab import drive
drive.mount('/content/drive', force_remount=True)

'''
!pip install -U torch torchvision ultralytics
'''



"""##  Library Import Significance

- **os, shutil, random**: For file operations and random sampling (e.g., train/test split).
- **cv2**: Image handling and preprocessing.
- **xml.etree.ElementTree**: Parsing XML annotations (Pascal VOC format).
- **google.colab.drive**: Mounts Google Drive in Colab for data access.
- **yaml**: Reads and writes YOLO configuration files.
- **ultralytics.YOLO**: Interface to train and evaluate YOLOv8 models.
- **time**: Measures execution time for training/inference.
"""

# Importing necessary Libraries
import os
import shutil
import random
import cv2
import xml.etree.ElementTree as ET
import random
import yaml
from ultralytics import YOLO
import time
from google.colab import drive
import matplotlib.pyplot as plt
import glob
from PIL import Image as PILImage
from IPython.display import display



"""## ðŸ§¹ Dataset Filtering and Splitting Logic

- **Data Validation**: The script verifies that each image is readable and has a corresponding, well-formed annotation with at least one object.
- **`is_valid_image`**: Checks if the image file can be loaded properly using OpenCV.
- **`is_valid_annotation`**: Ensures the annotation XML has at least one object tag.
- **Filtering**: `filter_clean_dataset` collects only valid image-annotation pairs, ignoring corrupt or incomplete data.
- **Data Splitting**: `split_from_train_to_train_and_test` moves a random percentage of valid pairs from `train/` to `test/`, ensuring both image and annotation files stay aligned.
- **Robust Workflow**: Useful for cleaning and preparing datasets for training object detection models like YOLO or Faster R-CNN.
"""

def is_valid_image(image_path):
    try:
        img = cv2.imread(image_path)
        return img is not None and img.size > 0
    except:
        return False

def is_valid_annotation(xml_path):
    try:
        root = ET.parse(xml_path).getroot()
        return root.find("object") is not None
    except:
        return False

def filter_clean_dataset(images_dir, annotations_dir):
    clean_files = []
    for filename in os.listdir(images_dir):
        if not filename.lower().endswith(('.jpg', '.jpeg', '.png')):
            continue

        base = os.path.splitext(filename)[0]
        image_path = os.path.join(images_dir, filename)
        xml_path = os.path.join(annotations_dir, f"{base}.xml")

        if os.path.exists(xml_path) and is_valid_image(image_path) and is_valid_annotation(xml_path):
            clean_files.append(filename)
        else:
            print(f"Skipping corrupt or incomplete pair: {filename}")

    print(f"\n Valid image-annotation pairs found: {len(clean_files)}")
    return clean_files

# Execute only once for data split
def split_from_train_to_train_and_test(image_train_dir, annot_train_dir, test_ratio=0.2):
    image_test_dir = image_train_dir.replace("train", "test")
    annot_test_dir = annot_train_dir.replace("train", "test")

    os.makedirs(image_test_dir, exist_ok=True)
    os.makedirs(annot_test_dir, exist_ok=True)

    clean_files = filter_clean_dataset(image_train_dir, annot_train_dir)
    random.shuffle(clean_files)

    test_count = int(len(clean_files) * test_ratio)
    test_files = clean_files[:test_count]

    print(f"\nMoving {test_count} image+annotation pairs to test/")

    for filename in test_files:
        base = os.path.splitext(filename)[0]
        img_src = os.path.join(image_train_dir, filename)
        xml_src = os.path.join(annot_train_dir, f"{base}.xml")

        img_dst = os.path.join(image_test_dir, filename)
        xml_dst = os.path.join(annot_test_dir, f"{base}.xml")

        shutil.move(img_src, img_dst)
        shutil.move(xml_src, xml_dst)

    print(" Test split completed.")


image_train_path = "/content/drive/MyDrive/road_damage_detection/dataset_augmented/images/train"
annot_train_path = "/content/drive/MyDrive/road_damage_detection/dataset_augmented/annotations/train"

split_from_train_to_train_and_test(image_train_path, annot_train_path, test_ratio=0.2)

"""# YOLO v8 Model

##  YOLO Dataset Preparation Script

- **Google Drive Integration**: Mounts the user's Google Drive to enable reading and writing files directly from Drive.
- **Directory Setup**: Creates the required folder structure (`images/train`, `images/test`, `labels/train`, `labels/test`) under a dedicated `dataset_yolo` directory.
- **Image Migration**: Copies training and testing images from the pre-augmented dataset (`dataset_augmented`) into the YOLO-specific structure.
- **XML to YOLO Conversion**:
  - Parses Pascal VOC-style XML annotations.
  - Converts bounding boxes into YOLO format: normalized center coordinates and dimensions.
  - Dynamically adds unknown classes if found, ensuring all object categories are captured.
- **Final Output**: Results in a clean YOLO-compatible dataset layout, ready for training with Ultralytics YOLO models.

 Essential preprocessing step for training object detection models efficiently with YOLO.
"""

#  Set up paths
BASE_PATH = "/content/drive/MyDrive/road_damage_detection"
AUG_IMAGE_DIR = f"{BASE_PATH}/dataset_augmented/images"
AUG_ANNOT_DIR = f"{BASE_PATH}/dataset_augmented/annotations"
YOLO_IMAGE_BASE = f"{BASE_PATH}/dataset_yolo/yolo/images"
YOLO_LABEL_BASE = f"{BASE_PATH}/dataset_yolo/yolo/labels"

for split in ['train', 'test']:
    os.makedirs(os.path.join(YOLO_IMAGE_BASE, split), exist_ok=True)
    os.makedirs(os.path.join(YOLO_LABEL_BASE, split), exist_ok=True)

# Move images from dataset_augmented to dataset_yolo
for split in ['train', 'test']:
    src_dir = os.path.join(AUG_IMAGE_DIR, split)
    dest_dir = os.path.join(YOLO_IMAGE_BASE, split)

    for file in os.listdir(src_dir):
        if file.endswith(".jpg"):
            shutil.copy(os.path.join(src_dir, file), os.path.join(dest_dir, file))

print("Images moved to YOLO folders.")

# Converting XML to YOLO format
damage_class_codes = {"D00": 0, "D01": 1, "D10": 2, "D11": 3,
                      "D20": 4, "D40": 5, "D43": 6, "D44": 7}
unknown_class = 8

def convert_xml_to_yolo(xml_path, label_output_dir, image_output_dir):
    global unknown_class
    root = ET.parse(xml_path).getroot()
    filename = root.find("filename").text
    image_src_path = os.path.join(image_output_dir, filename)
    if not os.path.exists(image_src_path):
        print(f" Skipping: Image not found for {filename}")
        return

    width = int(root.find("size/width").text)
    height = int(root.find("size/height").text)
    txt_path = os.path.join(label_output_dir, filename.replace(".jpg", ".txt"))

    with open(txt_path, "w") as f:
        for obj in root.findall("object"):
            cls = obj.find("name").text
            if cls not in damage_class_codes:
                damage_class_codes[cls] = unknown_class
                unknown_class += 1
            cls_id = damage_class_codes[cls]
            bbox = obj.find("bndbox")
            xmin = int(bbox.find("xmin").text)
            ymin = int(bbox.find("ymin").text)
            xmax = int(bbox.find("xmax").text)
            ymax = int(bbox.find("ymax").text)

            xc = ((xmin + xmax) / 2) / width
            yc = ((ymin + ymax) / 2) / height
            bw = (xmax - xmin) / width
            bh = (ymax - ymin) / height
            f.write(f"{cls_id} {xc:.6f} {yc:.6f} {bw:.6f} {bh:.6f}\n")

for split in ['train', 'test']:
    annot_dir = os.path.join(AUG_ANNOT_DIR, split)
    label_out = os.path.join(YOLO_LABEL_BASE, split)
    image_dir = os.path.join(YOLO_IMAGE_BASE, split)

    if os.path.exists(annot_dir):
        print(f" Converting {split} annotations...")
        for xml_file in os.listdir(annot_dir):
            if xml_file.endswith(".xml"):
                xml_path = os.path.join(annot_dir, xml_file)
                convert_xml_to_yolo(xml_path, label_out, image_dir)
        print(f" Done with {split} annotations.\n")

print(" YOLO dataset folder is ready in Google Drive.")


'''
!pip uninstall -y numpy
!pip install numpy==1.24.4
'''

"""## Creating YOLOv8 Data Configuration (data.yaml)

- **Defines the dataset structure** required by YOLOv8:
  - `path`: Base directory containing image and label folders.
  - `train` and `val`: Relative paths to training and validation image folders.
  - `nc`: Number of object classes in the dataset.
  - `names`: List of class names mapped by index.

- **Writes the configuration to** `data.yaml` in Google Drive, enabling YOLOv8 to locate and interpret the dataset for training and validation.
"""

yaml_content = """
path: /content/drive/MyDrive/road_damage_detection/dataset_yolo/yolo

train: images/train
val: images/test

nc: 10
names: ['D00', 'D01', 'D10', 'D11', 'D20', 'D40', 'D43', 'D44', 'D50', 'Repair']
"""

yaml_path = "/content/drive/MyDrive/road_damage_detection/dataset_yolo/data.yaml"

with open(yaml_path, "w") as f:
    f.write(yaml_content.strip())

print(f"Updated data.yaml saved at:\n{yaml_path}")

"""##  YOLOv8 Model Training Configuration

- **Model Initialization**: Loads a lightweight YOLOv8 architecture (`yolov8n.pt`) for object detection.
- **Data Path**: Uses a pre-defined `data.yaml` file to locate training and validation data.
- **Training Parameters**:
  - `epochs`: Trains for 50 full passes over the dataset.
  - `imgsz`: Resizes input images to 640Ã—640 for uniform processing.
  - `batch`: Processes 16 images per batch.
  - `augment`: Enables built-in data augmentations like mosaic and flipping to improve generalization.
  - `cache`: Speeds up training by caching images and labels in memory.
  - `workers`: Uses 2 parallel workers to load data efficiently.
  - `save`: Stores training checkpoints and logs.
- **Project Directory**: Saves all outputs under `yolo_runs/yolo_road_damage` in Google Drive.
- **Time Tracking**: Measures and prints total training time.
"""

BASE_PATH = "/content/drive/MyDrive/road_damage_detection"
yaml_path = f"{BASE_PATH}/dataset_yolo/data.yaml"

start = time.time()

model = YOLO("yolov8n.pt")

model.train(
    data=yaml_path,
    epochs=50,
    imgsz=640,
    batch=16,
    project=f"{BASE_PATH}/yolo_runs",
    name="yolo_road_damage",
    augment=True,
    cache=True,
    workers=2,
    save=True
)

end = time.time()
print(f"Training Time: {(end - start) / 60:.2f} minutes")



"""##  YOLOv8 Model Evaluation

- **Model Loading**: Loads the trained YOLOv8 model from the best saved weights.
- **Dataset Configuration**: Uses the `data.yaml` file to locate test dataset images and labels.
- **Evaluation**:
  - `val()`: Runs validation on the test set and computes detection metrics.
  - **Key Metrics Printed**:
    - `mAP@0.5`: Mean Average Precision at 0.5 IoU â€“ primary measure of detection quality.
    - `mAP@0.5:0.95`: Average over IoU thresholds from 0.5 to 0.95 â€“ stricter accuracy measure.
    - `Mean Precision`: Average precision across all classes.
    - `Mean Recall`: Average recall across all classes.
    - `Fitness Score`: Composite metric to estimate model quality by combining mAP, precision, and recall.

This evaluation helps compare models and understand performance on unseen data.
"""

BASE_PATH = "/content/drive/MyDrive/road_damage_detection"
WEIGHTS_PATH = f"{BASE_PATH}/yolo_runs/yolo_road_damage/weights/best.pt"
DATA_YAML = f"{BASE_PATH}/dataset_yolo/data.yaml"

model = YOLO(WEIGHTS_PATH)

metrics = model.val(data=DATA_YAML, split='val')

print("\n Evaluation Metrics:")
print(f"mAP@0.5:        {metrics.box.map50:.4f}")
print(f"mAP@0.5:0.95:   {metrics.box.map:.4f}")
print(f"Mean Precision: {metrics.box.mp:.4f}")
print(f"Mean Recall:    {metrics.box.mr:.4f}")
print(f"Fitness Score:  {metrics.box.fitness():.4f}")



"""##  YOLOv8 Evaluation Visualizations

This script automatically locates the latest YOLO evaluation results (e.g., from `runs/detect/val*`) and visualizes key performance plots to help assess model quality:

- **confusion_matrix.png**: Displays the confusion matrix showing true vs. predicted classes.
- **pr_curve.png**: Precision-Recall curve for understanding model performance across confidence thresholds.
- **F1_curve.png**: Visualizes the balance between precision and recall.

Each image is resized for clarity and displayed using matplotlib. These visualizations provide insights into class-wise accuracy, trade-offs, and overall model behavior.
"""

val_dir_root = "runs/detect"
val_subdirs = sorted([d for d in os.listdir(val_dir_root) if d.startswith("val")])
latest_val_dir = os.path.join(val_dir_root, val_subdirs[-1])

print(f"Using YOLO evaluation results from: {latest_val_dir}")

plot_files = ["confusion_matrix.png", "pr_curve.png", "F1_curve.png"]

for plot_file in plot_files:
    plot_path = os.path.join(latest_val_dir, plot_file)
    if os.path.exists(plot_path):
        img = cv2.imread(plot_path)
        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)

        plt.figure(figsize=(6, 4))
        plt.imshow(img)
        plt.axis('off')
        plt.title(plot_file)
        plt.show()



"""##  YOLOv8 Sample Inference Visualization

This script performs inference on 5 test images using a trained YOLOv8 model and displays the predictions:

- **Automatic Sampling**: Selects 5 test images from the YOLO test folder.
- **Model Inference**: Applies the YOLO model to detect and annotate objects.
- **Output Handling**: Saves each prediction result temporarily and resizes it for optimal display.
- **Visualization**: Uses PIL and IPython to visually inspect detection outputs in a compact format.

This helps quickly verify how well the model performs on unseen examples and spot-checks class-wise predictions.
"""

test_image_dir = f"{BASE_PATH}/dataset_yolo/yolo/images/test"
sample_imgs = sorted(glob.glob(test_image_dir + "/*.jpg"))[:5]

for img_path in sample_imgs:
    results = model(img_path)
    results[0].save(filename="/content/pred.jpg")


    img = PILImage.open("/content/pred.jpg")
    img.thumbnail((400, 400))
    display(img)



"""# Training the Faster R-CNN model using Detecron

## Installations for Detectron2 and TensorFlow Setup

- The first command installs the latest version of **Detectron2** directly from its official GitHub repository. Detectron2 is a powerful object detection and segmentation library developed by Facebook AI Research, built on top of PyTorch. Installing from source ensures you get the most updated features and bug fixes.

- The second command installs a compatible version of **TensorFlow (2.12.0)** along with necessary dependencies like `tensorflow-estimator`, `tf_slim`, and a specific `protobuf` version. These are sometimes required for compatibility with auxiliary tools or legacy support within object detection pipelines.

Together, these installations prepare your environment to work with both Detectron2 (for Faster R-CNN, Mask R-CNN, etc.) and TensorFlow-based models or utilities if needed.
"""





import os
import json
import xml.etree.ElementTree as ET
from tqdm import tqdm
import numpy as np
import random
import cv2
import matplotlib.pyplot as plt
import numpy as np
import seaborn as sns

from detectron2.data.datasets import register_coco_instances
from detectron2.engine import DefaultTrainer
from detectron2.config import get_cfg
from detectron2 import model_zoo
from detectron2.evaluation import COCOEvaluator, inference_on_dataset
from detectron2.data import build_detection_test_loader
from detectron2.utils.visualizer import Visualizer
from detectron2.utils.logger import setup_logger
from detectron2.engine import DefaultPredictor
from detectron2.data.catalog import MetadataCatalog
from pycocotools.cocoeval import COCOeval


from sklearn.metrics import confusion_matrix
from pycocotools.coco import COCO

"""##  XML to COCO Format Conversion for Road Damage Dataset

This script converts a dataset in Pascal VOC (XML) format into COCO JSON format, which is required for training and evaluating object detection models like Faster R-CNN in Detectron2.

###  Key Functionalities:
- **Directory Verification**: Ensures necessary annotation and image folders for both train and test splits exist.
- **Class Mapping**: Defines a consistent `name â†’ ID` mapping for 10 road damage classes.
- **Data Inspection**: Prints basic statistics to verify the number of annotations and images.
- **Conversion Logic**: Parses each XML file, extracts bounding boxes and class labels, and converts them into COCO-compliant dictionaries.
- **Output**: Saves separate `train.json` and `test.json` files in a designated output directory (`dataset_coco`) for direct use with COCO-based training pipelines.

This modular approach ensures clean, reproducible conversion for any downstream object detection workflow.
"""

BASE_PATH = "/content/drive/MyDrive/road_damage_detection"
ANNOT_TRAIN = f"{BASE_PATH}/dataset_augmented/annotations/train"
ANNOT_TEST = f"{BASE_PATH}/dataset_augmented/annotations/test"
IMG_TRAIN = f"{BASE_PATH}/dataset_augmented/images/train"
IMG_TEST = f"{BASE_PATH}/dataset_augmented/images/test"
COCO_DIR = f"{BASE_PATH}/dataset_coco"
COCO_TRAIN = os.path.join(COCO_DIR, "train.json")
COCO_TEST = os.path.join(COCO_DIR, "test.json")

class_name_to_id = {
    "D00": 0, "D01": 1, "D10": 2, "D11": 3,
    "D20": 4, "D40": 5, "D43": 6, "D44": 7,
    "D50": 8, "Repair": 9
}

for path in [ANNOT_TRAIN, ANNOT_TEST, IMG_TRAIN, IMG_TEST, COCO_DIR]:
    if not os.path.exists(path):
        os.makedirs(path, exist_ok=True)
        print(f" Created missing directory: {path}")
    else:
        print(f" Directory exists: {path}")

print("\n Data Summary:")
print("Train Annotations:", len(os.listdir(ANNOT_TRAIN)))
print("Test Annotations :", len(os.listdir(ANNOT_TEST)))
print("Train Images     :", len(os.listdir(IMG_TRAIN)))
print("Test Images      :", len(os.listdir(IMG_TEST)))

def xml_to_coco(xml_folder, img_folder, output_json_path, class_name_to_id):
    annotation_id = 1
    coco_dict = {
        "images": [],
        "annotations": [],
        "categories": [{"id": i, "name": name, "supercategory": "none"} for name, i in class_name_to_id.items()]
    }

    image_id = 1
    for xml_file in tqdm(sorted(os.listdir(xml_folder))):
        if not xml_file.endswith(".xml"):
            continue
        xml_path = os.path.join(xml_folder, xml_file)
        tree = ET.parse(xml_path)
        root = tree.getroot()

        filename = root.find("filename").text
        width = int(root.find("size/width").text)
        height = int(root.find("size/height").text)

        coco_dict["images"].append({
            "id": image_id,
            "file_name": filename,
            "width": width,
            "height": height
        })

        for obj in root.findall("object"):
            cls = obj.find("name").text
            if cls not in class_name_to_id:
                continue
            cls_id = class_name_to_id[cls]

            bbox = obj.find("bndbox")
            xmin = int(bbox.find("xmin").text)
            ymin = int(bbox.find("ymin").text)
            xmax = int(bbox.find("xmax").text)
            ymax = int(bbox.find("ymax").text)
            w = xmax - xmin
            h = ymax - ymin

            coco_dict["annotations"].append({
                "id": annotation_id,
                "image_id": image_id,
                "category_id": cls_id,
                "bbox": [xmin, ymin, w, h],
                "area": w * h,
                "iscrowd": 0
            })
            annotation_id += 1

        image_id += 1

    with open(output_json_path, "w") as f:
        json.dump(coco_dict, f, indent=4)
    print(f" COCO file saved to: {output_json_path}")

xml_to_coco(ANNOT_TRAIN, IMG_TRAIN, COCO_TRAIN, class_name_to_id)
xml_to_coco(ANNOT_TEST, IMG_TEST, COCO_TEST, class_name_to_id)



"""## Dataset Registration with Detectron2

This snippet registers the training and testing datasets in COCO format using Detectron2's `register_coco_instances` utility.

###  Key Points:
- Associates `train.json` and `test.json` with their respective image directories.
- Assigns dataset names `"rd_train"` and `"rd_test"` for use in model training and evaluation configs.
- Enables seamless integration with Detectron2's data loader, evaluator, and visualizer.

Registering datasets like this is essential for Detectron2 to recognize and access the data during training and inference workflows.
"""

register_coco_instances("rd_train", {}, "/content/drive/MyDrive/road_damage_detection/dataset_coco/train.json", "/content/drive/MyDrive/road_damage_detection/dataset_augmented/images/train")
register_coco_instances("rd_test", {}, "/content/drive/MyDrive/road_damage_detection/dataset_coco/test.json", "/content/drive/MyDrive/road_damage_detection/dataset_augmented/images/test")



"""##  Training Faster R-CNN with Detectron2

This script configures and launches training for a Faster R-CNN model using Detectron2 on a COCO-format road damage dataset.

### Key Highlights:
- **Model**: Uses a pre-configured Faster R-CNN with ResNet-50 and FPN from Detectron2â€™s model zoo.
- **Datasets**: `"rd_train"` and `"rd_test"` registered via COCO format.
- **Hyperparameters**:
  - `BASE_LR`: 0.00025
  - `MAX_ITER`: 7000
  - `IMS_PER_BATCH`: 2
  - `NUM_CLASSES`: 10 (based on road damage categories)
- **Output**: All checkpoints and logs are saved to `detectron_runs` in Google Drive.

###  Trainer:
Uses `DefaultTrainer`, which handles model loading, training loop, checkpointing, and evaluation with minimal custom code.
"""

cfg = get_cfg()
cfg.merge_from_file(model_zoo.get_config_file("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml"))

cfg.DATASETS.TRAIN = ("rd_train",)
cfg.DATASETS.TEST = ("rd_test",)
cfg.DATALOADER.NUM_WORKERS = 2
cfg.MODEL.WEIGHTS = model_zoo.get_checkpoint_url("COCO-Detection/faster_rcnn_R_50_FPN_3x.yaml")
cfg.SOLVER.IMS_PER_BATCH = 2
cfg.SOLVER.BASE_LR = 0.00025
cfg.SOLVER.MAX_ITER = 7000
cfg.MODEL.ROI_HEADS.BATCH_SIZE_PER_IMAGE = 128
cfg.MODEL.ROI_HEADS.NUM_CLASSES = 10
cfg.OUTPUT_DIR = "/content/drive/MyDrive/road_damage_detection/detectron_runs"

os.makedirs(cfg.OUTPUT_DIR, exist_ok=True)

trainer = DefaultTrainer(cfg)
trainer.resume_or_load(resume=False)
trainer.train()



"""## Evaluation and Visualization with Detectron2

This script evaluates the performance of a trained Faster R-CNN model using COCO-style metrics and visualizes predictions on sample test images.

###  Evaluation
- **Evaluator**: Uses `COCOEvaluator` on the `rd_test` dataset.
- **Metrics**: Includes mAP@0.5, mAP@0.5:0.95, and class-wise precision/recall.

###  Inference & Visualization
- **Predictor**: Utilizes `DefaultPredictor` for running inference on images.
- **Visualizer**: Overlays bounding boxes and class labels using Detectron2â€™s `Visualizer`.
- **Samples**: Randomly selects 3 images from the test set for demonstration.

###  Output
- Prints evaluation metrics.
- Displays prediction results with bounding boxes and labels using `matplotlib`.

This provides an end-to-end evaluation and visual check of detection performance on real data.
"""

setup_logger()

evaluator = COCOEvaluator("rd_test", cfg, False, output_dir=cfg.OUTPUT_DIR)
val_loader = build_detection_test_loader(cfg, "rd_test")
metrics = inference_on_dataset(trainer.model, val_loader, evaluator)
print("Evaluation Metrics:", metrics)

predictor = DefaultPredictor(cfg)
metadata = MetadataCatalog.get("rd_test")

test_dir = os.path.join(BASE_PATH, "dataset_augmented/images/test")
test_imgs = [f for f in os.listdir(test_dir) if f.endswith(".jpg")]
sample_imgs = random.sample(test_imgs, 3)

for img_name in sample_imgs:
    img_path = os.path.join(test_dir, img_name)
    img = cv2.imread(img_path)
    outputs = predictor(img)

    v = Visualizer(img[:, :, ::-1], metadata=metadata, scale=1.2)
    out = v.draw_instance_predictions(outputs["instances"].to("cpu"))

    plt.figure(figsize=(10, 6))
    plt.imshow(out.get_image()[:, :, ::-1])
    plt.title(img_name)
    plt.axis("off")
    plt.show()



"""##  Detectron2 Evaluation with COCO-style Metrics

This script evaluates a trained Faster R-CNN model using the COCO evaluation protocol and extracts detailed performance metrics.

###  Step-by-Step Breakdown

1. **COCOEvaluator**:
   - Evaluates the model on the `rd_test` dataset.
   - Computes mAP and class-wise metrics using Detectron2's built-in evaluator.

2. **Predictions**:
   - Loads predictions saved to `coco_instances_results.json`.

3. **Ground Truth Loading**:
   - Ground truth annotations are loaded from the `test.json` file using `pycocotools.COCO`.

4. **COCOeval**:
   - Initializes COCO evaluation for bounding boxes.
   - Computes and prints key metrics:
     - **mAP@0.5:0.95**
     - **mAP@0.5**
     - **Average Recall (AR)**
     - **Mean Precision and Recall**

###  Output
- Detailed evaluation including precision and recall at multiple IoU thresholds.
- Provides insights into model robustness across classes and bounding box sizes.
"""

evaluator = COCOEvaluator("rd_test", cfg, False, output_dir=cfg.OUTPUT_DIR)
val_loader = build_detection_test_loader(cfg, "rd_test")
metrics = inference_on_dataset(trainer.model, val_loader, evaluator)

ap = metrics['bbox']['AP']
ap50 = metrics['bbox']['AP50']

preds_path = os.path.join(cfg.OUTPUT_DIR, "coco_instances_results.json")
with open(preds_path, "r") as f:
    predictions = json.load(f)

gt_path = "/content/drive/MyDrive/road_damage_detection/dataset_coco/test.json"
coco_gt = COCO(gt_path)
coco_dt = coco_gt.loadRes(preds_path)

coco_eval = COCOeval(coco_gt, coco_dt, iouType='bbox')
coco_eval.evaluate()
coco_eval.accumulate()
coco_eval.summarize()

mean_precision = coco_eval.stats[0]
mean_recall = coco_eval.stats[8]



print("\n Evaluation Metrics (Detectron2 - Faster R-CNN):")
print(f"mAP@0.5:        {ap50:.4f}")
print(f"mAP@0.5:0.95:   {ap:.4f}")
print(f"Mean Precision: {mean_precision:.4f}")
print(f"Mean Recall:    {mean_recall:.4f}")
print(f"Fitness Score:  {(ap / 100 + mean_precision + mean_recall)/3:.4f}")



gt_path = "/content/drive/MyDrive/road_damage_detection/dataset_coco/test.json"
preds_path = "/content/drive/MyDrive/road_damage_detection/detectron_runs/coco_instances_results.json"

coco_gt = COCO(gt_path)

with open(preds_path, "r") as f:
    predictions = json.load(f)

cat_ids = coco_gt.getCatIds()
cat_id_to_idx = {cat_id: idx for idx, cat_id in enumerate(sorted(cat_ids))}
num_classes = len(cat_ids)
background_idx = num_classes

y_true, y_pred = [], []

for img_id in coco_gt.getImgIds():
    gt_ann_ids = coco_gt.getAnnIds(imgIds=img_id)
    gt_anns = coco_gt.loadAnns(gt_ann_ids)
    gt_labels = [cat_id_to_idx[ann['category_id']] for ann in gt_anns]

    dt_labels = [cat_id_to_idx[p['category_id']] for p in predictions if p['image_id'] == img_id and p['score'] > 0.5]

    max_len = max(len(gt_labels), len(dt_labels))
    gt_labels += [background_idx] * (max_len - len(gt_labels))
    dt_labels += [background_idx] * (max_len - len(dt_labels))

    y_true.extend(gt_labels)
    y_pred.extend(dt_labels)

cm = confusion_matrix(y_true, y_pred, labels=list(range(num_classes + 1)))

class_names = [cat["name"] for cat in coco_gt.loadCats(cat_ids)]
class_names.append("background")

plt.figure(figsize=(10, 8))
sns.heatmap(cm, annot=True, fmt="d", cmap="Blues", xticklabels=class_names, yticklabels=class_names)
plt.xlabel("Predicted")
plt.ylabel("True")
plt.title("Confusion Matrix - Detectron2 with Background")
plt.tight_layout()
plt.show()



"""##  YOLO vs Faster R-CNN: Metric Comparison Bar Chart

This script visualizes a side-by-side comparison of key performance metrics between two object detection models: **YOLOv8** and **Faster R-CNN**.

###  Metrics Compared
- **mAP@0.5**: Mean Average Precision at 0.5 IoU threshold.
- **mAP@0.5:0.95**: Average Precision across multiple IoU thresholds (COCO standard).
- **Precision**: Correct positive predictions over all positive predictions.
- **Recall**: Correct positive predictions over all ground truth positives.
- **Fitness Score**: Custom metric calculated as the average of mAP@0.5:0.95, precision, and recall.

###  Plot Description
- Few sets of bars represent YOLO and Faster R-CNN scores for each metric.
- Scores are annotated above each bar for clarity.
- A horizontal grid improves readability and comparison.

###  Insight
This visualization helps in **quickly assessing the trade-offs** between speed-focused (YOLO) and accuracy-focused (Faster R-CNN) models based on quantitative metrics.
"""



import matplotlib.pyplot as plt
import numpy as np
import time
import cv2
from detectron2.engine import DefaultPredictor



metrics = ["mAP@0.5", "mAP@0.5:0.95", "Precision", "Recall", "Fitness Score"]

yolo_values = [0.9241, 0.6621, 0.9123, 0.8542, 0.6883]
faster_rcnn_values = [0.7322, 0.4015, 0.4015, 0.5534, 0.4521]

x = np.arange(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(10, 6))
bars1 = ax.bar(x - width/2, yolo_values, width, label="YOLO")
bars2 = ax.bar(x + width/2, faster_rcnn_values, width, label="Faster R-CNN")

ax.set_ylabel("Score")
ax.set_title("Model Comparison: YOLO vs Faster R-CNN")
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()
ax.grid(True, axis='y', linestyle='--', alpha=0.7)

for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(bar.get_x() + bar.get_width() / 2, height),
                    xytext=(0, 3),
                    textcoords="offset points",
                    ha='center', va='bottom', fontsize=8)

plt.tight_layout()
plt.show()



"""##  YOLOv8 Inference Timing Script

This script measures the inference speed of a **YOLOv8 model** on a single image using OpenCV and the Ultralytics library.

###  Key Functions
- `time.time()`: Used before and after inference to compute elapsed time.
- `model(img)`: Runs inference on the input image using the loaded YOLO model.

###  Output Metrics
- **Inference Time** (in milliseconds): Measures how long the model takes to predict on one image.
- **FPS (Frames Per Second)**: Indicates how many images the model could theoretically process per second, helpful for evaluating real-time performance.

###  Use Case
This benchmark is crucial for assessing model suitability in **real-time applications** such as surveillance, autonomous driving, or mobile deployment.
"""

img_path = "/content/drive/MyDrive/road_damage_detection/dataset_augmented/images/test/aug_0_India_007629.jpg"
img = cv2.imread(img_path)

model = YOLO("/content/drive/MyDrive/road_damage_detection/yolo_runs/yolo_road_damage/weights/best.pt")

start = time.time()
results = model(img)
end = time.time()

inference_time = (end - start) * 1000
fps = 1 / (end - start)
print(f"ðŸ•’ YOLO Inference Time: {inference_time:.2f} ms  | FPS: {fps:.2f}")



"""##  Faster R-CNN Inference Timing Script (Detectron2)

This script evaluates the inference time of a **Faster R-CNN model** from the Detectron2 library on a single image.

###  Components
- `DefaultPredictor(cfg)`: Initializes a Detectron2 predictor with the given configuration.
- `predictor(img)`: Performs inference on the loaded image.
- `time.time()`: Used before and after prediction to calculate elapsed time.

###  Output Metrics
- **Inference Time**: Time taken to process one image (in milliseconds).
- **FPS (Frames Per Second)**: Frames the model can handle per second, indicating real-time readiness.

###  Purpose
Helps assess the **efficiency and latency** of the Detectron2 Faster R-CNN model for applications like live detection or embedded systems.
"""

predictor = DefaultPredictor(cfg)

start = time.time()
outputs = predictor(img)
end = time.time()

inference_time = (end - start) * 1000
fps = 1 / (end - start)
print(f" Faster R-CNN Inference Time: {inference_time:.2f} ms  | FPS: {fps:.2f}")



yolo_time_ms = 115.10
faster_rcnn_time_ms = 50.08


yolo_fps = 1000 / yolo_time_ms
faster_rcnn_fps = 1000 / faster_rcnn_time_ms


metrics = ['Inference Time (ms)', 'FPS']
yolo_values = [yolo_time_ms, yolo_fps]
faster_rcnn_values = [faster_rcnn_time_ms, faster_rcnn_fps]


x = np.arange(len(metrics))
width = 0.35

fig, ax = plt.subplots(figsize=(8, 5))
bars1 = ax.bar(x - width/2, yolo_values, width, label='YOLO', color='skyblue')
bars2 = ax.bar(x + width/2, faster_rcnn_values, width, label='Faster R-CNN', color='salmon')


ax.set_ylabel('Value')
ax.set_title('YOLO vs Faster R-CNN: Inference Time & FPS')
ax.set_xticks(x)
ax.set_xticklabels(metrics)
ax.legend()
ax.grid(True, axis='y', linestyle='--', alpha=0.5)


for bars in [bars1, bars2]:
    for bar in bars:
        height = bar.get_height()
        ax.annotate(f'{height:.2f}',
                    xy=(bar.get_x() + bar.get_width()/2, height),
                    xytext=(0, 3),
                    textcoords='offset points',
                    ha='center', va='bottom')

plt.tight_layout()
plt.show()

